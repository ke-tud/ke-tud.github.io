<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  </head>        
  <body>
    <pre>

%Aigaion2 BibTeX export from Knowledge Engineering Publications
%Thursday 23 September 2021 02:52:07 AM

@INPROCEEDINGS{jf:IJCNN-08,
        author = {Loza Menc{\'{\i}}a, Eneldo and F{\"{u}}rnkranz, Johannes},
         title = {Pairwise Learning of Multilabel Classifications with Perceptrons},
     booktitle = {Proceedings of the 2008 IEEE International Joint Conference on Neural Networks (IJCNN-08)},
          year = {2008},
         pages = {2900--2907},
  organization = {IEEE},
       address = {Hong Kong},
          isbn = {978-1-4244-1821-3},
           url = {http://www.ke.tu-darmstadt.de/publications/papers/loza08MLPP.pdf},
           doi = {10.1109/IJCNN.2008.4634206},
      abstract = {Multiclass multilabel perceptrons (MMP) have been proposed as an efficient incremental training algorithm for addressing a multilabel prediction task with a team of perceptrons.  The key idea is to train one binary classifier per label, as is typically done for addressing multilabel problems, but to make the training signal dependent on the performance of the whole ensemble. In this paper, we propose an alternative technique that is based on a pairwise approach, i.e., we incrementally train a perceptron for each pair of classes. Our evaluation on four multilabel datasets shows that the multilabel pairwise perceptron (MLPP) algorithm yields substantial improvements over MMP in terms of ranking quality and overfitting
resistance, while maintaining its efficiency. Despite the quadratic increase in the number of perceptrons that have to be trained, the increase in computational complexity is bounded by the average number of labels per training example.}
}

    </pre>
  </body>
</html>
